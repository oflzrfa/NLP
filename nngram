import nltk
import os
import datetime
import shutil
import random

documents_folder = 'C:/Users/refij/OneDrive/Dokumenti/GitHub/NLP/Articles/'

text_file_list = os.listdir(documents_folder)
text_files = [file_name for file_name in text_file_list if file_name.endswith('.txt')]
selected_files = random.sample(text_files, 20)

n = 5
output_folder = 'ngram_Articles'
os.makedirs(output_folder, exist_ok=True)

for file_name in selected_files:
    file_path = os.path.join(documents_folder, file_name)
    with open(file_path, 'r', encoding='utf-8') as file:
        text = file.read()
    # Remove non-alphabetic and non-whitespace characters
    clean_text = ''.join(filter(lambda c: c.isalpha() or c.isspace(), text.lower()))
    tokens = nltk.word_tokenize(clean_text)
    ngrams = list(nltk.ngrams(tokens, n))

    # Create a dictionary of n-gram frequencies
    freq_dict = {}
    for gram in ngrams:
        key = ' '.join(gram[:-1])
        if key not in freq_dict:
            freq_dict[key] = []
        freq_dict[key].append(gram[-1])

    generated_sentences = []
    minimum_sentences = 50
    maximum_sentences = 100
    num_sentences = random.randint(minimum_sentences, maximum_sentences)
    for _ in range(num_sentences):
        generated_text = ''
        start = ' '.join(random.choice(ngrams)[:-1])
        for _ in range(15):
            if start not in freq_dict:
                break
            next_word = random.choice(freq_dict[start])
            generated_text += next_word + ' '
            start = ' '.join(start.split()[1:]) + ' ' + next_word
        #generated_sentences.append(generated_text.capitalize() + '.')
        generated_sentences.append(generated_text.strip().capitalize() + '.')

    # Create a new text file for the generated sentences
    output_filename = f"{file_name}"
    output_path = os.path.join(output_folder, output_filename)
    with open(output_path, 'w', encoding='utf-8') as output_file:
        for sentence in generated_sentences:
            output_file.write(sentence + '\n')
